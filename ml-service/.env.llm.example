# ============================================================
# LLM Service Environment Configuration
# ============================================================

# Ollama Configuration (Local LLM - 100% FREE)
# Ollama runs locally on your machine, no API keys needed
OLLAMA_URL=http://localhost:11434

# Groq API Configuration (FREE TIER - OPTIONAL)
# Get free API key from: https://console.groq.com
# Free tier: 30 requests/minute, very fast inference
# Models: llama-3.1-70b-versatile (high quality, free)
GROQ_API_KEY=

# HuggingFace API Configuration (FREE TIER - OPTIONAL)
# Get free API key from: https://huggingface.co/settings/tokens
# Free tier: Inference API for various models
# Models: Mixtral-8x7B, Llama, etc.
HUGGINGFACE_API_KEY=

# ============================================================
# How to get FREE API keys (optional, for cloud fallback):
# ============================================================
#
# 1. GROQ (Recommended - Very Fast & Free):
#    - Visit: https://console.groq.com
#    - Sign up with email (free)
#    - Go to API Keys section
#    - Create new API key
#    - Paste above in GROQ_API_KEY
#    - Free tier: 30 req/min (very generous!)
#
# 2. HUGGINGFACE (Backup):
#    - Visit: https://huggingface.co/settings/tokens
#    - Sign up with email (free)
#    - Create new token (read access is enough)
#    - Paste above in HUGGINGFACE_API_KEY
#    - Free tier: Rate limited but works
#
# ============================================================
# Priority Order (automatic fallback):
# ============================================================
# 1. Ollama (local) - Always tried first, unlimited, free
# 2. Groq (cloud) - Fallback if Ollama down, 30 req/min free
# 3. HuggingFace (cloud) - Last resort backup
#
# You can run 100% locally with just Ollama (no API keys needed)!
